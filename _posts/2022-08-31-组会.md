---
title: 第一次组会
date: 2022-08-31
description: to sense the direction.
comments: true
mathjax: true
categories:
- Papers
tags:
- Meeting
---

# 会前

- 摘抄论文的排列组合

# 淋东-- Evolutionary Algorithm

代理模型：解决黑盒昂贵优化问题

进化算法

结构：

- 问题定义：结合EA框架
- 模型管理策略、搜索策略

搜索算子：

- 局部信息算子：在个体产生新解
- 全局信息算子：分布估计算法EDA，高斯模型、直方图模型
- 例子：

代理模型：

- 回归：连续
- 分类：离散
- 分类回归预筛选

summary：

全局局部：

![image-20220831194700680](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220831194700680.png)

算法架构：

![image-20220831194750109](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220831194750109.png)

挑选前n个最好的解，产生子代、评价

分类回归、预筛选

![image-20220831195244831](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220831195244831.png)

DE差分进化算法

EDA

![image-20220831195557985](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220831195557985.png)

CR-SAP

RBF

![image-20220831202109010](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220831202109010.png)

![image-20220831203335569](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220831203335569.png)

# 郑执——强化学习奖励稀疏

强化学习采样无效导致奖励稀疏，长时间才可得到奖励

原因：奖励迹太长

在马尔科夫中，对每个步骤进行采样

![image-20220901141432049](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220901141432049.png)

奖励迹太长，探索不常去的地方

warm up学习，这样可以解决奖励迹太长，方法：基于计数；基于差距

分层强化学习：选方向，选距离

## Curiosity

让Agent熟悉环境，通过熟悉程度评价State好坏

![image-20220901142031554](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220901142031554.png)

![image-20220901142056252](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220901142056252.png)

外层网络分方向，内层网络找点

内层网络训练快容易局部最优

半马尔科夫问题

## 离线采样迹太长

![image-20220901145554616](https://gitee.com/Acquent0/image-cloud/raw/master/img/image-20220901145554616.png)

## 资格迹有问题



